---
title: 고유 벡터(Eigenvectors), PCA, 공분산(Covariance) 및 엔트로피(Entropy)
layout: kr-default
---

# 고유 벡터(Eigenvectors), PCA, 공분산(Covariance) 및 엔트로피(Entropy)로의 초보자 가이드

내용: 

* [선형 변환(Linear Transformations)](#linear)
* [주성분 분석(Principal Component Analysis) (PCA)](#principal)
* [공분산 행렬(Covariance Matrix)](#covariance)
* [기본 변경(Change of Basis)](#change)
* [엔트로피(Entropy) & 정보 얻기(Information Gain)](#entropy)
* [코드를 주십시오(Just Give Me the Code)](#code)
* [리소스들](#resources)

이 포스팅은 고유 벡터들(eigenvectors)과 그들의 행렬들과의 관계를 쉬운 언어로 많은 수학 없이 소개합니다. 공분산(covariance), 주 구성 성분 분석(principal component analysis) 및 정보 엔트로피(information entropy)를 설명하는 것은 그 아이디어에 기반합니다. 

고유 벡터(eigenvector)의 *eigen*은 독일어 유래로, “very own(바로 자신의)”과 같은 무언가를 의미합니다. 예를 들어, 독일어로, “mein eigenes Auto”는 “my very own car(내 자신의 자동차)"를 의미합니다. 따라서 eigen은 두 가지 사이에서 특별한 관계를 나타냅니다. 특별한, 특징적이고 최종적인 무언가. 이 자동차, 또는 이 벡터는 나의 것이고, 다른 누군가의 것이 아닙니다.

선형 대수학에서 행렬들은 스프레드 시트와 같은 괄호들 사이의 스칼라(scalar) 값들의 모음인, 단순히 숫자들의 직사각형 어레이들 입니다. 모든 정방 행렬들은 (예를 들어, 2 x 2 또는 3 x 3) have 고유 벡터들을 가지고 있고, 그들은 고유 벡터들과, 독일인들의 그들의 자동차에 대한 것과 비슷한, 아주 특별한 관계를 가지고 있습니다.

## <a name="linear">선형 변환(Linear Transformations)</a>

저희는 행렬들이 무엇을 하는지, 그리고 그들이 어떻게 다른 숫자들과 관계하는지에 대한 간단한 우회 후 그 관계를 정의할 것 입니다.

행렬들은 유용합니다. 왜냐하면 여러분께서 그들로 더하기와 곱하기와 같은 것들을 할 수 있기 때문 입니다. 만약 한 벡터 *v*를 행렬 *A*로 곱하면, 여러분께서는 또다른 벡터 *b*를 얻고, 그 행렬은 입력 벡터 상에서 선형 대수학을 수행했다고 하실 수 있습니다. 

*Av = b* 

그것은 하나의 벡터 *v*에서 다른 벡터 *b*로 [매핑(maps)](https://en.wikipedia.org/wiki/Linear_map)합니다.  

저희는 구체적인 예제로 설명할 것 입니다. (여러분께서는 이런 유형의 행렬 곱셈이 dot product이라 불리는, 어떻게 수행되는지 [여기](http://math.stackexchange.com/a/24469)에서 보실 수 있습니다.)

![Alt text](../img/eigen_matrix.png)

따라서 *A*는 *v*를 *b*로 변환합니다. 아래의 그래프에서, 여러분께서는 어떻게 그 행렬이 짧고 낮은 선인 *v*이 길고 높은 선안 *b*에 매핑되었는지 보실 수 있습니다. 

![Alt text](../img/two_vectors.png)

여러분은 하나의 positive 벡터를 연이어 행렬 A으로 공급하실 수 있고, 각각은 오른쪽으로 더 높고, 멀게 뻗어 새로운 공간으로 투영됩니다. 

모든 입력 벡터 *v*가 이와 같은 정상 그리드(normal grid)에서 살고 있다고 상상해 보십시오:

![Alt text](../img/space_1.png)

그리고 그 행렬이 그들 모두를 출력 벡터 *b*를 보유하고 있는, 하나 아래의 새로운 공간으로 투영합니다:

![Alt text](../img/space_2.png)

여기에서 여러분은 병치된 그 두개의 공간들을 보실 수 있습니다:

![Alt text](../img/two_spaces.png)

(*제공: William Gould, Stata Blog*)

그리고 여기에 하나의 공간에서 다른 곳으로 변형하는 행렬의 작업을 보여주는 애니메이션이 있습니다:

<iframe src="https://upload.wikimedia.org/wikipedia/commons/0/06/Eigenvectors.gif" width="100%" height="300px;" style="border:none;"></iframe>

파란색 선들은 고유 벡터 입니다. 

여러분께서는 가시적인 결과를 생산하는 보이지 않는 힘, 바람의 돌풍과 같은 행렬을 상상하실 수 있습니다. 그리고 바람의 돌풍은 어떤 한 방향으로 불어야만 합니다. 그 고유 벡터는 그 행렬이 불어오는 방향을 여러분께 알려줍니다.

![Alt text](../img/mona_lisa_eigenvector.png)
(*제공: Wikipedia*)

따라서, 한 공간을 통해 불어오는 행렬에 의해 영향을 받는 모든 벡터들 중, 어떤 벡터가 그 고유 벡터 일까요? 그것은 방향을 바꾸지 않는 그 벡터 입니다; 다시 말해, 그 고유 벡터는 이미 그 행렬이 모든 벡터들을 밀어내는 것과 동일한 방향으로 향하고 있습니다. 고유 벡터는 풍향계(weathervane)와 같은 것 입니다. 마치 eigenvane 처럼. 

그러므로 고유 벡터의 정의는 그 행렬이 스칼라 상관 계수인 것 처럼 행렬에 반응하는 벡터 입니다. 이 방정식에서, A는 행렬, x 벡터, lambda 스칼라 상관 계수, 마치 5 또는 37 또는 pi와 같은 숫자 입니다.

![Alt text](../img/lambda_eigen.png)

여러분께서는 또한 고유 벡터들은, 입력 벡터들을 스트레칭 하거나 압축하는, 어떤 선형 변환이 작동하는 지에 따르는 축들(axes)이라고 할 수 있습니다. 그들은 선형 변환에서 바로 그 “행”인 더 큰 행렬의 작동을 표현하는 변화의 행들(lines) 입니다.

저희가 복수형을 사용하는 것을 눈치채셨을 것 입니다 – 축들(axes) 및 행들(lines). 마치 독일인이 식료품 쇼핑을 위해 폭스바겐을, 비즈니스 출장을 위해 메르세데스를, 재미 주행을 위해 포르쉐를 가지고 있는 것 처럼 (각각은 별개의 목적을 제공), 정방 행렬은 그들이 차원들(dimensions)을 가지고 있는 만큼 고유 벡터들을 가질 수 있습니다; 예를 들어, 2 x 2 행렬은 두개의 고유 벡터들을, 3 x 3 행렬은 세개의, 그리고 n 행렬은 n개의 고유 벡터들을 가질 수 있습니다. 각각의 하나는 하나의 차원에서 그것의 선의 활동을 나타냅니다.[1](#ref) 

왜냐하면 고유 행렬들은 하나의 행렬이 입력을 따라 움직이는 주된 힘의 축들을 증류하기 때문에 그들은 행렬 분해에 유용합니다; 즉, [행렬의 고유 벡터들과 함께 그의  diagonalization](http://mathworld.wolfram.com/MatrixDiagonalization.html). 그 고유 벡터들이 그 행렬의 대표이기 때문에 그들은 오토인코더들이 딥 신경망에 의해 구축한 것과 같은 동일한 작업을 수행합니다. 

Yoshua Bengio를 이용하면:

    많은 수학적 개체들은, 그들을 구성 성분들로 나누거나, 우리가 그들을 표현하려고 선택한 방법에 의해서 발생하지 않은, 보편적인 일부의 속성들을 찾아냄으로써 더 잘 이해될 수 있습니다.
    
    예를 들어, 정수(integers)들은 소인수(prime factors)들로 분해될 수 있습니다. 우리가 숫자 12를 표현하는 방법은 우리가 그것을 십진법으로 혹은 이진법으로 작성할 것인지에 따라 달라집니다, 그러나 12 = 2 × 2 × 3은 항상 진실 입니다. 
    
    이 표현으로부터 우리는 12는 5로 나누어질 수 없다거나 12의 어떤 정수배도 3으로 나누어질 수 있다는 것과 같은 유용한 properties을 결론지을 수 있습니다.
    
    정수를 소인수들로 분해함으로써 정수의 본질에 대한 무언가를 많이 발견함에 따라, 우리는 또한 우리에게 한 어레이의 요소들로부터 명확하지 않은 그들의 기능적인 properties에 대한 정보를 보여준 방식들로 행렬들을 분해할 수 있습니다.
    
    가장 널리 사용된 행렬 분해 중 하나는 하나의 행렬을 한 세트의 고유 행렬과 고유 값으로 분해하는 eigen-decomposition라 불리는 것 입니다.

## <a name="principal">주성분 분석(Principal Component Analysis) (PCA)</a> 

PCA는 이미지와 같은 고차원의 데이터에서 패턴을 찾는 한 도구 입니다. 머신-학습 실무자들은 종종 그들의 신경망을 위한 데이터를 전처리하기 위해서 PCA를 사용합니다. 데이터를 센터링, 로테이팅 및 스케일링 함으로써, PCA는 차원(dimensionality)을 우선순위화 하고 (여러분께서 몇몇의 저분산 차원들을 제거하도록) 그 신경망의 수렴 속도와 결과의 전반적인 질을 향상시킬 수 있습니다. 

To get to PCA에 대해 이해하기 위해, 저희는 신속히 몇몇의 기본적인 통계 개념들을 정의할 것 입니다 -- mean*, standard deviation, variance* 및 *covariance* -- 그래서 저희가 그들을 나중에 함께 엮을 수 있도록. 그들의 방정식은 밀접하게 관련되어 있습니다. 

*Mean(평균)은* 단순히 모든 데이터 점들의 합계를 데이트 점들의 수, *n*으로 나눔으로써 발견되는 세트 X에서 모든 *x*의 평균적인 값 입니다.

![Alt text](../img/mean.png)

*Standard deviation(표준 편차)*, 그 발음만큼 재미있는, 단순히 데이터 점들의 평균에 대한 평균 제곱 거리의 제곱근 입니다. 아래의 방정식에서, 분자는 각각의 데이터점과 그 평균 사이의 차이들의 합을 포함하고, 분모는 단순히 평균 거리를 제공하는 데이터 점들의 수 (-1) 입니다.

![Alt text](../img/standard_deviation.png)

Variance(분산)은 그 데이터의 속도 측정 입니다. 만약 제가 [네덜란드 농구 팀 선수들](http://www.theguardian.com/world/2015/apr/08/scientists-try-to-answer-why-dutch-people-are-so-tall)을 가지고 그들의 키를 측정한다면, 그 측정치들은 많은 분산을 가지지 않을 것 입니다. 그들은 모두 6 feet (183 cm) 이상으로 그룹화될 것 입니다.

그러나 만약 제가 그 네덜란드 농구 팀을 정신병 유치원의 한 교실에 던져 넣는다면, 그 합쳐진 그룹의 키 측정치들은 많은 분산을 가질 것 입니다. 분산은 펼쳐진, 또는 데이터가 표현하는 차이의 양 입니다.

분산은 단순히 표준 편차의 제곱이고, 종종 *s^2*로 표현됩니다.

![Alt text](../img/variance.png)

분산과 표준 편차 둘 모두에서 데이터 점들과 그 평균 사이의 차이를 제곱하는 것은, 그들을 양수로 만들고 평균 이상 및 이하의 값들이 서로를 상쇄하지 않도록 합니다.

여러분께서 그 개인들의 나이 (x 축)와 키 (y 축)를  기입하고 (그 평균을 0로 설정) 직사각형의 분산표를 얻었다고 가정해보도록 하겠습니다: 

![Alt text](../img/scatterplot.png)

PCA는 데이터를 통해 선형 회귀 분석과 같은 직선의, 설명적인 선들을 그리고자 합니다. 

각각의 직선은 한 “주성분 요소(principal component),” 또는 독립 및 종속 변수 사이의 관계를 표현합니다. 데이터에서 차원들이 존재하는 만큼 주성분이 존재하는 반면, PCA의 역할은 그들을 우선화하는데 있습니다. 

첫번째 주성분은 대부분의 분산을 설명하는 방식으로 직선으로 산점도를 이등분 합니다; 즉, 그것은 가장 긴 차원의 데이터를 따릅니다. (이것은 적색 라인들에 의해 표현된 최소의 에러와 일치하게 됩니다…) 아래의 그래프에서 “바게뜨”를 길이로 잘라냅니다.

![Alt text](../img/scatterplot_line.png)

두번째 주성분은 첫번째에 의해 만들어진 에러들을 피팅할 그 첫번째에 대해 수직으로 데이터를 자릅니다. 단지 두개의 주성분들이 위의 그래프에 있지만, 만약 3차원이라면 세번째 주성분은 첫번째와 두번째 주성분으로부터의 에러들을 피팅할 것 이고, 이는 계속 됩니다.

## <a name="covariance">공분산 행렬(Covariance Matrix)</a>

저희가 한 세트의 벡터들을 다른 것들로 변환한 무언가로서 행렬들을 소개했던 반면, 그들에 대해 생각할 다른 방식은 작업 시 힘(forces)을 캡쳐하는 데이터의 설명으로서 입니다. 두개의 변수들이 그들의 분산과 공분산에 의해 표현된 대로 서로에게 관련된다는 것에 의한 힘.

저희가 데이터의 분산, 그리고 변수들 사이의 공분산을 설명하는 정방 행렬의 숫자를 구성하는 것을 상상해보십시오. 이것이 *공분산 행렬(covariance matrix)* 입니다. 그것이 저희가 관찰하는 데이터의 경험적 설명 입니다.

공분산의 고유 벡터와 고유 값을 찾는 것은 직선의, 주성분 행들을 데이터의 분산으로 피팅하는 것과 동일합니다. 이유는? 왜냐하면 고유 벡터들은 *힘의 주 행들을 추적(trace the principal lines of force)*하고, 최고의 분산 및 공분산의 축들이 그 데이터가 어디에서 가장 변화에 민감한지를 보여주기 때문 입니다.

그것을 이처럼 생각해보십시오: 한 변수가 변화하면, 그것은 알려진 또는 알려지지 않은 힘에 의해 작동됩니다. 두개의 변수들이 함께 변화하면, 하나가 다른 것에 작동하기 때문에, 혹은 그들 모두가 동일한 숨겨진 및 익명의 힘에 종속되기 때문일 것 입니다.

한 행렬이 선형 변환을 수행할 때, 고유 벡터는 그의 입력에 적용하는 힘의 행들을 추적합니다; 한 행렬이 분산과 공분산으로 채워질 때, 고유 벡터는 그 주어진 힘에 적용되어져 온 힘을 반영합니다. 하나가 힘을 적용하고, 다른 것은 그것을 반영합니다.

*Eigenvalues(고유 값)*은 단순히 축들에 범위를 주는 고유 벡터들에 부착된 상관 계수들 입니다. 이 경우, 그들은 그 데이터의 공분산의 측정 입니다. 그들의 고유 값을 가장 높은 것에서 가장 낮은 것 순서대로 여러분의 고유 벡터들을 순서 매김으로써, 여러분은 중요성 순서대로 주성분을 얻게 됩니다.

2 x 2 행렬을 위한 공분산 행렬은 이와 같을 것 입니다:

![Alt text](../img/covariance_matrix.png)

좌측 하단 및 우측 상단의 동일한 숫자들이 x와 y 사이의 공분산을 표현하는 반면, 좌측 상단과 우측 하단에 있는 숫자들은 x 및 y 변수의 분산을 각각 표현합니다. 그 정체성 때문에, 이런 행렬들은 대칭으로 알려져 있습니다. 여러분께서 보시는 바와 같이, PCA 섹션의 상단 근처의 그래프는 위로, 그리고 오른쪽으로 향하기 때문에, 공분산은 양수 입니다.

두개의 변수들이 함께 증가하고 감소하면 (위로, 오른쪽으로 가는 행), 그들은 양의 공분산을 가지고, 하나가 증가하고 다른 하나가 감소하면, 그들은 음의 공분산을 가집니다 (아래로, 오른쪽으로 가는 행). 

![Alt text](../img/covariances.png)

(*제공: Vincent Spruyt*)

하나의 변수 또는 다른 변수가 전혀 움직이지 않고, 그 그래프가 대각선의 움직임을 보이지 않을 때, 어떠한 공분산도 없다는 것을 기억하십시오. 공분산이 이 질문에 답할 것 입니다: 두 변수들은 함께 움직이는가? 하나가 움직이는 반면, 다른 것이 움직이지 않는 다면, 그 답은, 아니다 입니다.

또한 아래의 방정식에서, 여러분께서는 공분산과 분산 사이에 단지 작은 차이가 있다는 것을 보실 수 있습니다.

![Alt text](../img/covariance.png)

vs.

![Alt text](../img/variance.png)

공분산을 계산하는 것에 대한 좋은 점은, 여러분께서 상관 변수적인 관계들을 볼 수 없는 높은 차원 공간에서, 두개의 변수들이 그들의 공분산의 양, 음, 비존재적인 캐릭터에 의해 함께 움직이는 방법을 알 수 있다는 것 입니다.(*Correlation(상관 관계)*는 -1과 1 사이의 값으로, 정규화된 공분산의 한 종류 입니다.)

To sum up, 요약하면, 공분산 행렬은 데이터의 형태를 정의합니다. x-및-y-축-정렬된 스프레드는 분산에 의해 표현되는 반면, 고유 벡터를 따르는 대각선 스프레드는 공분산에 의해 표현됩니다. 

통계학에서 인과 관계는 나쁜 이름 입니다. 따라서 가감해서 이를 취하시기 바랍니다: 

완전히 정확하지는 않지만, 위의 네덜란드 농구 선수 예제에서, 인과적 힘으로서 각각의 성분을 생각하는 것이 도움이 될 것 입니다. 첫번째 주 성분으로 나이를; 두번째로 성별을; 세번째로 국적을 (국가별 다른 의료 시스템을 의미), 그리고 키에 관련하여 그 자신의 차원을 점유하는 것들의 각각으로. 각각은 키에서 다른 정도로 작동합니다. 여러분께서는 가능한 원인의 추적으로서 공분산을 읽을 수 있습니다.

### <a name="change">기본 변경(Change of Basis)</a>

공분산 행렬의 고유 벡터들이 각자에게 직교하기 때문에, 그들은 x 및 y 축들로부터의 데이터를 주 구성들에 의해 표현된 축들로 방향을 전환하는데 사용될 수 있습니다. 여러분은 그의 가장 큰 분산의 행들에 의해 정의된 새로운 공간에서 데이터 세트를 위한 [좌표계를 리베이스(re-base the coordinate system)](https://en.wikipedia.org/wiki/Change_of_basis) 합니다.

위에서 저희가 보여드려 온 x 및 y 축들은 행렬의 기본(기저)이라 불리는 것 입니다; 즉, 그들은 x, y 좌표들로 행렬의 점들을 제공합니다. 그러나 다른 축들을 따라 행렬을 개조하는 것이 가능합니다; 예를 들어, 한 행렬의 고유 벡터들이 동일한 행렬을 위한 새로운 세트의 좌표들의 기반으로서 역할을 할 수 있습니다. 행렬들과 벡터들은, x 및 y와 같은 특정한 좌표 시스템에 링크된 숫자들과는 독립적인, 그들 자신들 입니다.

![Alt text](../img/basis_change.png)

In the graph above위의 그래프에서, 저희는 동일한 벡터 v가 두개의 좌표 시스템들, 검정의 x-y 축들, 그리고 적색 점선으로 봉지는 두개의 다른 축들 에서 어떻게 다르게 자리할 수 있는지를 보여드립니다. 첫번째 좌표 시스템에서, v = (1,1), 그리고 두번째에서, v = (1,0), 그러나 v 그 자체는 변하지 않았습니다. 그러므로 벡터들과 행렬들은 괄호들 내에서 보여지는 숫자들로부터 추출될 수 있습니다. 

이것은 심오하고, 거의 영적인 의미들을 가지고 있습니다. 그 중 하나는 자연 좌표계가 존재하지 않고, n 차원의 공간에 있는 수학적인 객체들은 복수의 설명들에 종속된다는 것 입니다.(행렬들의 베이스들을 변경하는 것은 그들을 더 쉽게 조작하게 합니다.) 

벡터들을 위한 기본 변경은 숫자들을 위한 베이스를 변경하는 것과 대략 유사합니다; 예를 들어, 수량 9는 십집법에서 9로, 이진법에서 1001로, 그리고 삼진법에서 30으로서 설명됩니다. (예를 들면, 1, 2, 10, 11, 12, 20, 21, 22, 30 <-- 이것이 “9” 입니다). 동일한 수량, 다른 기호; 동일한 벡터, 다른 좌표.

## <a name="entropy">엔트로피 & 정보 얻기(Entropy & Information Gain)</a>

정보 이론에서, 용어 *엔트로피(entropy)*는 우리가 가지고 있지 않은 정보를 의미합니다 (일반적으로 사람들은 그들이 알고 있는 것대로 “정보(information)”를 정의하고, 전문 용어는 그의 머리에 일반 언어를 초보자의 손해로 바꾸는 데에 다시 한번 승리했습니다). 우리가 한 시스템에 대해 가지고 있지 않는 정보, 그것의 엔트로피는 그것의 예측불가능성에 관련되어 있습니다: 그것이 우리를 얼마나 놀라게할 수 있는지.

만약 여러분께서 어떤 동전이 양면에 머리를 가지고 있다는 것을 안다면, 동전 던지기는 여러분께 전혀 정보를 제공하지 않습니다. 왜냐하면 그것은 매번 머리를 줄 것이기 때문 입니다. 여러분은 알고자 동전을 던질 필요가 없습니다. 저희는 두개의 머리를 가진 동전은 어떤 정보도 가지고 있지 않다고 말할 것 입니다. 왜냐하면 그것은 여러분을 놀라게 할 어떤 방식도 가지고 있지 않기 때문 입니다.

균형잡힌, 양면의 동전은 매번 동전을 던질 때 놀라움의 요소를 포함합니다. 그리고 동일한 증명으로, 여섯 면의 주사위는 각각의 던지기에 동일한 횟수로서 여섯개의 결과 중 어떤 하나를 제공할 더 많은 놀라움을 포함합니다. 둘 모두의 객체들은 기술적인 의미에서 *정보*를 포함합니다.

이제 주사위가 던져져, 여섯번의 굴리기 중 다섯번의 경우 “3”이 나왔고, 우리가 그 게임이 조작된 것을 알게되었다고 상상해보십시오. 갑자기 이 주사위의 각각의 굴리기로 제공된 놀라움의 양이 급격히 줄어듭니다. 우리는 주사위의 행동에서 우리에게 더 큰 예측 능력을 제공하는 한 경향을 이해합니다. 

주사위 굴리기를 이해하는 것은 데이터 세트에서 주 성분을 찾는 것과 유사합니다. 여러분께서는 단순히 기본 패턴을 확인합니다.

시스템에 대해 *우리가 알지 못하는 것(what we don't know)*으로부터 우리가 아는 것으로의 정보의 그 전송은 엔트로피에서 한 변화를 의미 합니다. 통계는 시스템의 엔트로피를 감소시킵니다. 정보를 얻으면, 엔트로피는 감소합니다. 이것이 정보 얻기(information gain) 입니다. 그리고 맞습니다, 엔트로피의 이 유형이 우리의 손 안에 있는 시스템에 대해 우리가 아는 것에 의존한다는 점에서 주관적입니다. (가치있는 정보로서, [정보 얻기](https://en.wikipedia.org/wiki/Information_gain_in_decision_trees)는 저희가 [제한된 볼츠만 머신(restricted Boltzmann machines)](../kr-restrictedboltzmannmachine.html)상의 이 튜토리얼에서 간단히 살펴본, Kullback-Leibler divergence와 동의어 입니다.) 

그래서 산점도를 자른 각각의 주 성분은 그 시스템의 엔트로피에서, 그의 예측불가능성에서, 감소를 표현합니다.

데이터의 형태에게 대부분의 분산을 책임지는 성분으로 시작하는 때에 하나의 주 성분을 설명하는 것은 데이터를 결정 트리(decision tree)를 따르는 것과 유사합니다. 정상적으로 형성된 결정 트리에 있는 첫번째 if-then-else 분류와 같은 PCA의 첫번째 성분은 예측불가능성을 가장 많이 감소하는 차원에 따른 것 입니다.

## <a name="code">코드를 주십시오</a>

여러분께서는 저희가 Numpy에 의해 광범위하게 영감을 얻은 n 차원의 어레이들을 처리하는 JVM을 위한 숫자 컴퓨팅 라이브러리인 [ND4J](http://kr-nd4j.org)에서 어떻게 [고유 벡터](https://github.com/deeplearning4j/nd4j/blob/master/nd4j-api/src/main/java/org/nd4j/linalg/eigen/Eigen.java)를 하는지를 보실 것 입니다. ND4J는 Java 및 [Scala](https://github.com/deeplearning4j/nd4s) APIs를 가지고 있고, Hadoop 및 Spark 상에서 실행하며, 매우 큰 행렬 연산 상에서 대략 [Numpy/Cython보다 2배 정도 빠르며](http://nd4j.org/benchmarking).

## <a name="resources">다른 리소스들</a>

* [주 성분 분석(Principal Components Analysis) 튜토리얼](http://www.cs.otago.ac.nz/cosc453/student_tutorials/principal_components.pdf)
* [고유 값/고유 벡터의 중요성은 무엇인가?](http://math.stackexchange.com/a/23325)
* [PCA, 고유 벡터 및 고유 값 이해하기](http://stats.stackexchange.com/a/140579/85518)
* [공분산 행렬의 기하학적 해석](http://www.visiondummy.com/2014/04/geometric-interpretation-covariance-matrix/)
* [고유 벡터 & 고유 값으로의 안내 Part 1](https://www.youtube.com/watch?v=G4N8vJpf7hM) (Video)
* [(또 다른) 고유 벡터 & 고유 값으로의 안내](https://www.youtube.com/watch?v=8UX82qVJzYI) (Video)
* [정보 & 엔트로피, 단위 1, 강의 2](https://www.youtube.com/watch?v=phxsQrZQupo) (MIT OpenCourseWare)
* [250억원의 고유 벡터: 구글 뒤의 선형 대수학](https://www.rose-hulman.edu/~bryan/googleFinalVersionFixed.pdf)


## <a name="resources">초보자용 다른 가이드들</a>

* [회귀 분석 & 신경망](../linear-regression.html)
* [Word2vec: 가공되지 않은 텍스트로부터 관계 추출하기](../kr-word2vec.html)
* [제한된 볼츠만 머신(Restricted Boltzmann Machines): Deep-Belief Networks의 구축된 블럭](../kr-restrictedboltzmannmachine.html)
* [재발성 네트워크 및 긴 단기 메모리 단위](../lstm.html)

<a name="ref">1)</a> *일부의 경우들에서, 행렬들은 전체 세트의 고유 벡터들을 가지지 않을 수 있습니다; 그들은 최대한 그들 각각의 순서, 또는 차원의 수만큼, 선형적으로 독립적인 고유 벡터들을 가질 것 입니다.*
