---
title: 
layout: default
---

# stacked denoising autoencoder

A [stacked denoising autoencoder](http://deeplearning.net/tutorial/SdA.html) is to a denoising autoencoder what a deep-belief network is to a [restricted Boltzmann machine](../restrictedboltzmannmachine.html). A key function of SDAs, and deep learning more generally, is their capacity for unsupervised pre-training, layer by layer, as input is fed through. Once each layer is pre-trained to conduct feature selection and extraction on the input from the preceding layer, a second stage of supervised fine tuning can follow. 

A word on stochastic corruption in SDAs: Denoising autoencoders shuffle data around and learn about that data by attempting to reconstruct it. The act of shuffling is the noise, and the job of the network is to recognize the features within the noise that will allow it to classify the input. When a network is being trained, it generates a model, and measures the distance between that model and the benchmark through a loss function. Its attempts to minimize the loss function involve resampling the shuffled inputs and re-reconstructing the data, until it finds those inputs which bring its model closest to what it has been told is true. 

The serial resamplings are based on a generative model to randomly provide data to be processed. This is known as a [Markov chain](https://en.wikipedia.org/wiki/Markov_chain#Steady-state_analysis_and_limiting_distributions), and more specifically, a [Markov chain Monte Carlo](https://en.wikipedia.org/wiki/Markov_chain_Monte_Carlo) algorithm that steps through the data set seeking a representative sampling of indicators that can be used to construct more and more complex features.

### parameters

See the [parameters common to all multilayer networks](../multinetwork.html).

#### corruption level 

The amount of noise to apply to the input will take the form of a percentage. Typically 30 percent (0.3) is fine, but if you have a small amount of data, you may want to consider adding more.

### initiating a stacked denoising autoencoder

Here's how you set up a single-thread stacked denoising autoencoder: 

To create it, you simply instantiate an object of the [class](../doc/org/deeplearning4j/sda/StackedDenoisingAutoEncoder.html).

		    double pretrain_lr = 0.001;
			double corruption_level = 0.3;
			int pretraining_epochs = 50;
			double finetune_lr = 0.001;
			int finetune_epochs = 50;
			int test_N = 2;
			RandomGenerator rng = new JDKRandomGenerator();
			int n_ins = 2;
			int n_outs = 2;
			int[] hidden_layer_sizes_arr = {300,200,100};
			int n_layers = hidden_layer_sizes_arr.length;



		    DataSet xor = MatrixUtil.xorData(100, 200);


		    StackedDenoisingAutoEncoder sda = new StackedDenoisingAutoEncoder.Builder()
				.hiddenLayerSizes(hidden_layer_sizes_arr)
				.numberOfInputs(xor.getFirst().columns).numberOfOutPuts(n_outs)
				.useRegularization(false).withMomentum(0.).withRng(rng).build();


This creates a stacked denoising autoencoder with the specified parameters. Note the 0.5 corruption level. A higher corruption level is better for smaller data sets. The number of inputs is based on the columns of the input data set generated by xor (100). The number of outputs in this case is two -- true or false -- for the xor function.

You can test your newly trained network by feeding it unstructured data and checking the output. The output predicts whether the specified input is true or false according to xor rules.


			DoubleMatrix predict = dbn.predict(xor.getFirst());

			Evaluation eval = new Evaluation();
			eval.eval(y, predict);
			System.out.println(eval.stats());


This will print out the f1 score of the prediction.

Note that the eval class combines [confusion matrices](../glossary.html#confusionmatrix) and f1 scores to allow for easy display and evaluation of data by allowing input of outcome matrices. This is useful for tracking how well your network trains over time. 

The f1 score will be a percentage. It's basically the probability that your guess are correct correct. Eighty-six percent is industry standard; a solid deep-learning network should be capable of scores in the high 90s.

If you run into trouble, try modifying the hidden layer sizes, and tweaking other parameters to get the f1 score up.

Next, we'll show you how to use [distributed and multithreaded computing](../scaleout.html) to train your networks more quickly.